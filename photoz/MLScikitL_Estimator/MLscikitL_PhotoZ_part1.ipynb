{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src = figs/logos/logo-IJCLab_v1.png height=40, style=\"padding: 10px\"> \n",
    "<b>PhotoZ estimation with scikit learn Machine learning </b> <br>\n",
    "Last verified to run on 2022-03-19 with LSST Science Pipelines release w_2021_49 <br>\n",
    "Contact authors: Sylvie Dagoret-Campagne (DP0 Delegate) <br>\n",
    "Target audience: DP0 delegates member <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credit:** Originally developed by Sylvie Dagoret-Campagne in the framework provided by Rubin DP0.1 (reference DP0.1 tutorials)\n",
    "\n",
    "Acknowledgement: Melissa Graham, Leanne Guy, Alex Drlica-Wagner, Keith Bechtol, Grzegorz Madejski, Louise Edwards, and many others .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives : Compare PhotoZ estimators performances using simple Machine Learning algorithm from scikit learn.\n",
    "\n",
    "Three typical regressors in scikit learn are evaluated and compared together. Finaly those are compared to the CMNN Photo-Z estimator (introduced in a previous notebook).\n",
    "No optimisation is performed in this notebook. This will be the subject of another complementary notebook.\n",
    "\n",
    "This notebook comes in two parts:\n",
    "\n",
    "- Part 1 : Selection of the dataset, including discussion on photometric detection\n",
    "- Part 2 : Comparison of Ridge, RandomForest, Gradient Boosting estimators and CMNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Note:** : \n",
    "- all plots are made with Holoview.\n",
    "- **Better select the maximum of CPU.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general python packages\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pandas.testing import assert_frame_equal\n",
    "import os\n",
    "import errno\n",
    "import shutil\n",
    "import getpass\n",
    "import datetime\n",
    "# Import the Rubin TAP service utilities\n",
    "from lsst.rsp import get_tap_service, retrieve_query\n",
    "\n",
    "# LSST Science Pipelines (Stack) packages\n",
    "import lsst.daf.butler as dafButler\n",
    "import lsst.afw.display as afwDisplay\n",
    "import lsst.geom as geom\n",
    "import lsst.afw.coord as afwCoord\n",
    "afwDisplay.setDefaultBackend('matplotlib')\n",
    "\n",
    "#\n",
    "from lsst import skymap\n",
    "\n",
    "# Astropy\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.units.quantity import Quantity\n",
    "from astropy.visualization import (MinMaxInterval, SqrtStretch,ZScaleInterval,PercentileInterval,\n",
    "                                   ImageNormalize,imshow_norm)\n",
    "from astropy.visualization.stretch import SinhStretch, LinearStretch,AsinhStretch,LogStretch\n",
    "\n",
    "\n",
    "# Bokeh for interactive visualization\n",
    "import bokeh\n",
    "from bokeh.io import output_file, output_notebook, show\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import ColumnDataSource, CDSView, GroupFilter, HoverTool\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import factor_cmap\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import streams, opts\n",
    "from holoviews.operation.datashader import rasterize\n",
    "from holoviews.operation.datashader import datashade, dynspread\n",
    "from holoviews.plotting.util import process_cmap\n",
    "\n",
    "import datashader as dsh\n",
    "\n",
    "\n",
    "# Set the maximum number of rows to display from pandas\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "\n",
    "# Set the holoviews plotting library to be bokeh\n",
    "# You will see the holoviews + bokeh icons displayed when the library is loaded successfully\n",
    "#hv.extension('bokeh')\n",
    "hv.extension('bokeh', 'matplotlib')\n",
    "\n",
    "# Display bokeh plots inline in the notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What versions of bokeh and holoviews nd datashader are we working with?\n",
    "# This is important when referring to online documentation as\n",
    "# APIs can change between versions.\n",
    "print(\"Bokeh version: \" + bokeh.__version__)\n",
    "print(\"Holoviews version: \" + hv.__version__)\n",
    "print(\"Datashader version: \" + dsh.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  What version of the Stack are we using?\n",
    "! echo $IMAGE_DESCRIPTION\n",
    "! eups list -s | grep lsst_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow for matplotlib to create inline plots in our notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt      # imports matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import warnings                      # imports the warnings library\n",
    "import gc                            # imports python's garbage collector\n",
    "\n",
    "# Ignore warnings\n",
    "from astropy.units import UnitsWarning\n",
    "warnings.simplefilter(\"ignore\", category=UnitsWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some plotting defaults:\n",
    "\n",
    "params = {'axes.labelsize': 28,\n",
    "          'font.size': 24,\n",
    "          'legend.fontsize': 14,\n",
    "          'xtick.major.width': 3,\n",
    "          'xtick.minor.width': 2,\n",
    "          'xtick.major.size': 12,\n",
    "          'xtick.minor.size': 6,\n",
    "          'xtick.direction': 'in',\n",
    "          'xtick.top': True,\n",
    "          'lines.linewidth': 3,\n",
    "          'axes.linewidth': 3,\n",
    "          'axes.labelweight': 3,\n",
    "          'axes.titleweight': 3,\n",
    "          'ytick.major.width': 3,\n",
    "          'ytick.minor.width': 2,\n",
    "          'ytick.major.size': 12,\n",
    "          'ytick.minor.size': 6,\n",
    "          'ytick.direction': 'in',\n",
    "          'ytick.right': True,\n",
    "          'figure.figsize': [18, 10],\n",
    "          'figure.facecolor': 'White'\n",
    "          }\n",
    "\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools to explain the expected detected magnitudes distributions\n",
    "\n",
    "- taken from from https://github.com/ixkael/Photoz-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(m) = m^\\alpha \\exp\\left( - \\left(m/m_{max}\\right)^\\beta \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_mag(imag_grid,maglim):\n",
    "    \"\"\"   \n",
    "    \n",
    "    Model of magnitude distribution in photometric survey\n",
    "    \n",
    "    p_mag(imag_grid,maglim)\n",
    "    from https://github.com/ixkael/Photoz-tools\n",
    "    \n",
    "    input args:\n",
    "     - imag_grid : magnitude\n",
    "     - maglim limit of magnitude\n",
    "     \n",
    "     return the probability of magnitude distribution\n",
    "     \n",
    "     THIS IS THE MODEL THAT MUST BE USED\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # some parameters for prob(imagnitude)\n",
    "    alpha = 15.0 \n",
    "    beta = 2\n",
    "    off=1\n",
    "\n",
    "    # prob(imagnitude) distribution\n",
    "    p_imag = imag_grid**alpha*np.exp(-(imag_grid/(maglim-off))**beta)\n",
    "    p_imag /= p_imag.sum()\n",
    "    return p_imag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imag errir distribution as function of mag limit, as in Rykoff et al\n",
    "def imag_err(m, mlim):\n",
    "    \"\"\"\n",
    "    from https://github.com/ixkael/Photoz-tools\n",
    "    \"\"\"\n",
    "    \n",
    "    a, b = 4.56, 1\n",
    "    k = 1\n",
    "    sigmadet = 5\n",
    "    teff = np.exp(a + b * (mlim - 21.))\n",
    "    F = 10**(-0.4*(m-22.5))\n",
    "    Flim = 10**(-0.4*(mlim-22.5))\n",
    "    Fnoise = (Flim/sigmadet)**2 * k * teff - Flim\n",
    "    return 2.5/np.log(10) * np.sqrt( (1 + Fnoise/F) / (F*k*teff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_prob(imag_grid,maglim):\n",
    "    \"\"\"\n",
    "    det_prob(imag_grid,maglim)\n",
    "    \n",
    "    Give detection probability of a magnitude\n",
    "    from https://github.com/ixkael/Photoz-tools\n",
    "    \n",
    "    input arg:\n",
    "    - imag_grid : magnitude grid\n",
    "    - maglim limit of magnitude\n",
    "    \n",
    "    return histogram of magnitude probability\n",
    "    \n",
    "    THIS IS THE MODEL THAT MUST BE USED\n",
    "    \n",
    "    \"\"\"\n",
    "    pp_mag=p_mag(imag_grid,maglim)\n",
    "    \n",
    "    detprob = 1*pp_mag \n",
    "    ind = (imag_grid >= maglim - 0.4)\n",
    "    #detprob[ind] *= ( 1 - scipy.special.erf((imag_grid[ind]-maglim+0.4)/0.4) )\n",
    "    # detection probability looks like a sigmoid\n",
    "    detprob[ind] *= np.exp( -0.5*((imag_grid[ind]-maglim+0.4)/0.2)**2)\n",
    "    detprob /= detprob.sum() * (imag_grid[1]-imag_grid[0])\n",
    "    return detprob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations and initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holoview Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HV_CURVE_SINGLE_WIDTH  = 400\n",
    "HV_CURVE_SINGLE_HEIGHT = 350\n",
    "HV_CURVE_MULTI_WIDTH  = 300\n",
    "HV_CURVE_MULTI_HEIGHT = 300\n",
    "HV_CURVE_MULTI_FRAME_WIDTH = 300\n",
    "HV_CURVE_MULTI_COLS   = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_HISTO = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HV_HISTO_SINGLE_WIDTH  = 600\n",
    "HV_HISTO_SINGLE_HEIGHT = 600\n",
    "HV_HISTO_MULTI_WIDTH  = 300\n",
    "HV_HISTO_MULTI_HEIGHT = 300\n",
    "HV_HISTO_MULTI_FRAME_WIDTH = 300\n",
    "HV_HISTO_MULTI_COLS   = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HV_IMAGE_SINGLE_WIDTH  = 400\n",
    "HV_IMAGE_SINGLE_HEIGHT = 400\n",
    "HV_IMAGE_SINGLE_FRAME_WIDTH = 600\n",
    "HV_IMAGE_MULTI_WIDTH  = 400\n",
    "HV_IMAGE_MULTI_HEIGHT = 400\n",
    "HV_IMAGE_MULTI_FRAME_WIDTH = 300\n",
    "HV_IMAGE_MULTI_COLS   = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# username\n",
    "myusername=getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary folders if necessary\n",
    "NBDIR       = 'photoz_part1'                           # relative path for this notebook output\n",
    "TMPTOPDIR   = \"/scratch\"                               # always write some output in /scratch, never in user HOME \n",
    "TMPUSERDIR  = os.path.join(TMPTOPDIR,myusername)       # defines the path of user outputs in /scratch \n",
    "TMPNBDIR    = os.path.join(TMPUSERDIR,NBDIR)           # output path for this particular notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create user temporary directory\n",
    "if not os.path.isdir(TMPUSERDIR):\n",
    "    try:\n",
    "        os.mkdir(TMPUSERDIR)\n",
    "    except:\n",
    "        raise OSError(f\"Can't create destination directory {TMPUSERDIR}!\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create this notebook temporary directory\n",
    "if not os.path.isdir(TMPNBDIR):\n",
    "    try:\n",
    "        os.mkdir(TMPNBDIR)\n",
    "    except:\n",
    "        raise OSError(f\"Can't create destination directory {TMPNBDIR}!\" ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Defines steering flags and parameters\n",
    "\n",
    "The Output queries may be saved in files if requested. \n",
    "By defaults all the following flags are set False : no query output is saved in file.\n",
    "To speed-up the demo, the presenter may keep some of those flags True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAG_WRITE_DATAFRAMEONDISK  = True                     # Select of query output will be saved on disk\n",
    "FLAG_READ_DATAFRAMEFROMDISK = True                     # Select of the query can be red from disk if it exists\n",
    "FLAG_CLEAN_DATAONDISK       = False                     # Select of the output queries saved in file will be cleaned at the end of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Retrieve source from catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coadds-Magnitude cutoff for 10 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAX,GMAX,RMAX,IMAX,ZMAX,YMAX = 26.1, 27.4, 27.5, 26.8, 26.1,24.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection function in catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to build a query passing a coordinate and a search radius\n",
    "def getQueryCircle(c: SkyCoord, r: Quantity) -> str:\n",
    "    query = \"SELECT obj.ra, obj.dec, obj.objectId, obj.extendedness, \"\\\n",
    "            \"obj.mag_u_cModel, obj.mag_g_cModel, obj.mag_r_cModel, \"\\\n",
    "            \"obj.mag_i_cModel, obj.mag_z_cModel, obj.mag_y_cModel, \"\\\n",
    "            \"obj.magerr_u_cModel, obj.magerr_g_cModel, obj.magerr_r_cModel, \"\\\n",
    "            \"obj.magerr_i_cModel, obj.magerr_z_cModel, obj.magerr_y_cModel, \"\\\n",
    "            \"truth.truth_type, truth.redshift, truth.match_objectId \" \\\n",
    "            \"FROM dp01_dc2_catalogs.object as obj \" \\\n",
    "            \"JOIN dp01_dc2_catalogs.truth_match as truth \" \\\n",
    "            \"ON truth.match_objectId = obj.objectId \" \\\n",
    "            \"WHERE CONTAINS(POINT('ICRS', obj.ra, obj.dec),\"\\\n",
    "            \"CIRCLE('ICRS', \" + str(c.ra.value) + \", \" + str(c.dec.value) + \", \" \\\n",
    "            + str(r.to(u.deg).value) + \" )) = 1 \" \\\n",
    "            \"AND obj.good = 1 \"  \\\n",
    "            \"AND truth.match_objectid >= 0 \" \\\n",
    "            \"AND truth.is_good_match = 1 \" \\\n",
    "            \"AND truth.truth_type = 1 \" \\\n",
    "            \"AND obj.mag_u_cModel - 5*obj.magerr_u_cModel < \" +str(UMAX) +\" \"\\\n",
    "            \"AND obj.mag_g_cModel - 5*obj.magerr_g_cModel < \" +str(GMAX) +\" \"\\\n",
    "            \"AND obj.mag_r_cModel - 5*obj.magerr_r_cModel < \" +str(RMAX) +\" \"\\\n",
    "            \"AND obj.mag_i_cModel - 5*obj.magerr_i_cModel < \" +str(IMAX) +\" \"\\\n",
    "            \"AND obj.mag_z_cModel - 5*obj.magerr_z_cModel < \" +str(ZMAX) +\" \"\\\n",
    "            \"AND obj.mag_y_cModel - 5*obj.magerr_y_cModel < \" +str(YMAX)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and initialisation of the Rubin TAP Service client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an instance of the TAP service\n",
    "service = get_tap_service()\n",
    "assert service is not None\n",
    "assert service.baseurl == \"https://data.lsst.cloud/api/tap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a reference position on the sky  for a square seach\n",
    "c1 = SkyCoord(ra=62.0*u.degree, dec=-40.*u.degree, frame='icrs')\n",
    "size = 1.0 * u.deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = getQueryCircle(c1, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_result=f'cat_photozpart1_result.pkl'\n",
    "fullfilename_result=os.path.join(TMPNBDIR,filename_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection flags\n",
    "\n",
    "- put boolean flags here to avoid execution of some sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plots on redshift distribution\n",
    "FLAG_SHOW_TRUE_REDSHIFT_DISTRIB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plots to check the photometry selected for photoz\n",
    "# For a pure demo on photoZ, this section can be skipped\n",
    "FLAG_SHOW_PHOTOMETRY_DETECTION = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To speed up the database query, especially if you run this notebook in the context of a DP0 demo public session,\n",
    "you can copy my data file **cat_photozpart1_result.pkl** from **/scratch/sylvielsstfr/photoz_part1/cat_photozpart1_result.pkl** to your path **/scratch/yourusername/photoz_part1**\n",
    "\n",
    "- check the variable **FLAG_READ_DATAFRAMEFROMDISK=True**\n",
    "- your username is given by **yourusername=getpass.getuser()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_READ_DATAFRAMEFROMDISK and os.path.exists(fullfilename_result):\n",
    "    sql_result = pd.read_pickle(fullfilename_result)\n",
    "else:\n",
    "    job = service.submit_job(query)\n",
    "    job.run()\n",
    "    job.wait(phases=['COMPLETED', 'ERROR'])\n",
    "    print('Job phase is', job.phase)\n",
    "    #sql_result = job.fetch_result().to_table().to_pandas()\n",
    "    \n",
    "    \n",
    "    # Create and submit the job. This step does not run the query yet\n",
    "    job = service.submit_job(query)\n",
    "    # Get the job URL\n",
    "    print('Job URL is', job.url)\n",
    "\n",
    "    # Get the job phase. It will be pending as we have not yet started the job\n",
    "    print('Job phase is', job.phase)\n",
    "    \n",
    "    # Run the job. You will see that the the cell completes executing,\n",
    "    # even though the query is still running\n",
    "    job.run()\n",
    "    \n",
    "    # Use this to tell python to wait for the job to finish if\n",
    "    # you don't want to run anything else while waiting\n",
    "    # The cell will continue executing until the job is finished\n",
    "    job.wait(phases=['COMPLETED', 'ERROR'])\n",
    "    print('Job phase is', job.phase)\n",
    "    \n",
    "    # A usefull funtion to raise an exception if there was a problem with the query\n",
    "    job.raise_if_error()\n",
    "    \n",
    "    # Once the job completes successfully, you can fetch the results\n",
    "    async_data = job.fetch_result()\n",
    "    \n",
    "    sql_result = async_data.to_table().to_pandas()\n",
    "    \n",
    "    \n",
    "if FLAG_WRITE_DATAFRAMEONDISK:\n",
    "    sql_result.to_pickle(fullfilename_result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sql_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for shorter names\n",
    "data.rename(columns={\"mag_u_cModel\": \"mag_u\", \"mag_g_cModel\": \"mag_g\",\"mag_r_cModel\": \"mag_r\",\n",
    "                     \"mag_i_cModel\": \"mag_i\", \"mag_z_cModel\": \"mag_z\",\"mag_y_cModel\": \"mag_y\",\n",
    "                     \"magerr_u_cModel\": \"magerr_u\", \"magerr_g_cModel\": \"magerr_g\",\"magerr_r_cModel\": \"magerr_r\",\n",
    "                     \"magerr_i_cModel\": \"magerr_i\", \"magerr_z_cModel\": \"magerr_z\",\"magerr_y_cModel\": \"magerr_y\",\n",
    "                    },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output directory where the extracted catalog is temporary saved\n",
    "! ls -l $TMPNBDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map truth_type\n",
    "data['truth_type']=data['truth_type'].map({1: 'galaxy', 2: 'star', 3: 'SNe'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop objects that are not galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop objects that are not galaxies\n",
    "data.drop(data.loc[data['truth_type'] != 'galaxy' ].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NA\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "data[\"umg\"]=data[\"mag_u\"]- data[\"mag_g\"]\n",
    "data[\"gmr\"]=data[\"mag_g\"]- data[\"mag_r\"]\n",
    "data[\"rmi\"]=data[\"mag_r\"]- data[\"mag_i\"]\n",
    "data[\"imz\"]=data[\"mag_i\"]- data[\"mag_z\"]\n",
    "data[\"zmy\"]=data[\"mag_z\"]- data[\"mag_y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ckeck input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshifts distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_TRUE_REDSHIFT_DISTRIB:\n",
    "    (z_bin, count) = np.histogram(data.redshift, bins=NBINS_HISTO)\n",
    "    z_distribution = hv.Histogram(z_bin, count).opts(title=f\"redshift distribution\",color='darkblue', \n",
    "    xlabel='redshift', fontscale=1.2,\n",
    "    height=HV_HISTO_SINGLE_HEIGHT-100, width=HV_HISTO_SINGLE_WIDTH,tools=['hover'])\n",
    "    \n",
    "    z_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnitudes distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle of photodetection\n",
    "\n",
    "Study of a simple model to find the expected magnitude distribution including photodetection bias:\n",
    "\n",
    "- blue curve : the true magnitude distribution\n",
    "- green curve : the detected magnitude distribution\n",
    "- red : detection threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymag_grid = np.linspace(17,30,100)\n",
    "mymaglim = 27\n",
    "mymag0 = 17\n",
    "mymag1 = 30\n",
    "prob_mag_tab = p_mag(mymag_grid,mymaglim)\n",
    "dens_mag_tab = prob_mag_tab/np.sum(prob_mag_tab)/(mymag_grid[1]-mymag_grid[0])\n",
    "det_prob_tab =  det_prob(mymag_grid,mymaglim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_opts = dict(\n",
    "                xaxis=\"bottom\", \n",
    "                padding = 0.01, fontsize={'title': '8pt'},\n",
    "                height=HV_CURVE_SINGLE_HEIGHT, width=HV_CURVE_SINGLE_WIDTH+100,tools=['hover']\n",
    "               )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    curve_magdist = hv.Curve(zip(mymag_grid,dens_mag_tab),label=\"true mag\").opts(**curve_opts).opts(color=\"blue\") \n",
    "    curve_maglim = hv.VLine(mymaglim,label=\"detection threshold\").opts(color=\"red\")\n",
    "    curve_prob = hv.Curve(zip(mymag_grid,det_prob_tab),label=\"detected mag\").opts(**curve_opts).opts(color=\"green\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    layout = (curve_magdist * curve_prob * curve_maglim).opts(legend_position='top_left',xlabel=\"magnitude\",ylabel=\"probability\",title=\"magnitude density distribution\")\n",
    "    layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude distribution before cutoff at detection threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The catalog sources have been extracted $5\\sigma$ above the cutoff. This allow the user to refine the expected cutoff inside each band "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection cutiff definition\n",
    "maglim_u = hv.VLine(UMAX).opts(color=\"magenta\")\n",
    "maglim_g = hv.VLine(GMAX).opts(color=\"magenta\")\n",
    "maglim_r = hv.VLine(RMAX).opts(color=\"magenta\")\n",
    "maglim_i = hv.VLine(IMAX).opts(color=\"magenta\")\n",
    "maglim_z = hv.VLine(ZMAX).opts(color=\"magenta\")\n",
    "maglim_y = hv.VLine(YMAX).opts(color=\"magenta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo_opts = dict(\n",
    "                xaxis=\"bottom\", \n",
    "                padding = 0.01, fontsize={'title': '8pt'},\n",
    "                height=HV_HISTO_MULTI_HEIGHT, width=HV_HISTO_MULTI_WIDTH,tools=['hover']\n",
    "               )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    mag_bin, count = np.histogram(data.mag_u, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magu = hv.Histogram(mag_bin, count).opts(title=f\"mag U\",xlabel=\"U\",color='blue').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_g, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magg = hv.Histogram(mag_bin, count).opts(title=f\"mag G\",xlabel=\"G\",color='green').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_r, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magr = hv.Histogram(mag_bin, count).opts(title=f\"mag R\",xlabel=\"R\",color='red').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_i, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magi = hv.Histogram(mag_bin, count).opts(title=f\"mag I\",xlabel=\"I\",color='orange').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_z, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magz = hv.Histogram(mag_bin, count).opts(title=f\"mag Z\",xlabel=\"Z\",color='grey').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_y, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magy = hv.Histogram(mag_bin, count).opts(title=f\"mag Y\",xlabel=\"Y\",color='black').opts(**histo_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    layout1 = h_magu * maglim_u + h_magg * maglim_g + h_magr * maglim_r + h_magi *  maglim_i + h_magz *  maglim_z + h_magy *  maglim_y\n",
    "    layout1.cols(HV_HISTO_MULTI_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Photometric detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The magnitudes in DC2 correcpond to average magnitudes and their photometric error. In the telescope, the detection correspond to the realisation of one sample of these distribution.\n",
    "Then the cutoff will be applied on detected magnitudes. (But the evaluation of photoz algorithm will be applied on the average magnitudes in the catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### randomisation of magnitude with photometric errors\n",
    "\n",
    "- gaussian distribution should be applied to flux, not magnitudes. Assume central limit theorem applies on magnitudes as well. This could be checked or corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photodet_mag(mag,errmag):\n",
    "    return np.random.normal(loc=mag, scale=errmag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['mag_u_det'] = data.apply(lambda x:  photodet_mag(x['mag_u'], x['magerr_u']), axis=1)\n",
    "data['mag_g_det'] = data.apply(lambda x:  photodet_mag(x['mag_g'], x['magerr_g']), axis=1)\n",
    "data['mag_r_det'] = data.apply(lambda x:  photodet_mag(x['mag_r'], x['magerr_r']), axis=1)\n",
    "data['mag_i_det'] = data.apply(lambda x:  photodet_mag(x['mag_i'], x['magerr_i']), axis=1)\n",
    "data['mag_z_det'] = data.apply(lambda x:  photodet_mag(x['mag_z'], x['magerr_z']), axis=1)\n",
    "data['mag_y_det'] = data.apply(lambda x:  photodet_mag(x['mag_y'], x['magerr_y']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection on detected magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photodet_select(mu,mg,mr,mi,mz,my):\n",
    "    return (mu>17) and (mu < UMAX) and (mg < GMAX) and (mr < RMAX) and (mi < IMAX) and (mz < ZMAX) and (my < YMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['selected'] = data.apply(lambda x:  photodet_select(x['mag_u_det'], x['mag_g_det'], x['mag_r_det'], x['mag_i_det'],x['mag_z_det'], x['mag_y_det'] ), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"selected\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('selected', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of average magnitudes after photometric detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    mag_bin, count = np.histogram(data.mag_u, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magu = hv.Histogram(mag_bin, count).opts(title=f\"mag U\",xlabel=\"U\",color='blue').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_g, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magg = hv.Histogram(mag_bin, count).opts(title=f\"mag G\",xlabel=\"G\",color='green').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_r, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magr = hv.Histogram(mag_bin, count).opts(title=f\"mag R\",xlabel=\"R\",color='red').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_i, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magi = hv.Histogram(mag_bin, count).opts(title=f\"mag I\",xlabel=\"I\",color='orange').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_z, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magz = hv.Histogram(mag_bin, count).opts(title=f\"mag Z\",xlabel=\"Z\",color='grey').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_y, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magy = hv.Histogram(mag_bin, count).opts(title=f\"mag Y\",xlabel=\"Y\",color='black').opts(**histo_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    layout2 = h_magu * maglim_u + h_magg * maglim_g + h_magr * maglim_r + h_magi *  maglim_i + h_magz *  maglim_z + h_magy *  maglim_y\n",
    "    layout2.cols(HV_HISTO_MULTI_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of detected magnitudes after photometric detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    mag_bin, count = np.histogram(data.mag_u_det, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magud = hv.Histogram(mag_bin, count).opts(title=f\"mag U\",xlabel=\"U\",color='blue').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_g_det, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_maggd = hv.Histogram(mag_bin, count).opts(title=f\"mag G\",xlabel=\"G\",color='green').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_r_det, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magrd = hv.Histogram(mag_bin, count).opts(title=f\"mag R\",xlabel=\"R\",color='red').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_i_det, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magid = hv.Histogram(mag_bin, count).opts(title=f\"mag I\",xlabel=\"I\",color='orange').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_z_det, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magzd = hv.Histogram(mag_bin, count).opts(title=f\"mag Z\",xlabel=\"Z\",color='grey').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_y_det, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magyd = hv.Histogram(mag_bin, count).opts(title=f\"mag Y\",xlabel=\"Y\",color='black').opts(**histo_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    layout3 = h_magud * maglim_u + h_maggd * maglim_g + h_magrd * maglim_r + h_magid *  maglim_i + h_magzd *  maglim_z + h_magyd *  maglim_y\n",
    "    layout3.cols(HV_IMAGE_MULTI_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detected magnitude vs true magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    huudet, xhuz, yhuz=np.histogram2d(data.mag_u,data.mag_u_det,bins=(50, 50),range=[[16,28],[16,28]])\n",
    "    hggdet, xhgz, yhgz=np.histogram2d(data.mag_g,data.mag_g_det,bins=(50, 50),range=[[16,28],[16,28]])\n",
    "    hrrdet, xhrz, yhrz=np.histogram2d(data.mag_r,data.mag_r_det,bins=(50, 50),range=[[16,28],[16,28]])\n",
    "    hiidet, xhiz, yhiz=np.histogram2d(data.mag_i,data.mag_i_det,bins=(50, 50),range=[[16,28],[16,28]])\n",
    "    hzzdet, xhzz, yhzz=np.histogram2d(data.mag_z,data.mag_z_det,bins=(50, 50),range=[[16,28],[16,28]])\n",
    "    hyydet, xhyz, yhyz=np.histogram2d(data.mag_y,data.mag_y_det,bins=(50, 50),range=[[16,28],[16,28]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_opts = dict(\n",
    "                #height=600, width=700, \n",
    "                xaxis=\"bottom\", \n",
    "                padding = 0.01, fontsize={'title': '8pt'},\n",
    "                #colorbar=True, toolbar='right', show_grid=True,\n",
    "                #aspect='equal',\n",
    "                frame_width= HV_HISTO_MULTI_FRAME_WIDTH,\n",
    "                show_grid=True ,\n",
    "                #ylabel=\"mag\",\n",
    "                tools=['hover','undo','redo','zoom_in','zoom_out'],\n",
    "                #tools=[myhover,'crosshair'],\n",
    "               )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    fhuudet=np.flipud(huudet.T)\n",
    "    fhggdet=np.flipud(hggdet.T)\n",
    "    fhrrdet=np.flipud(hrrdet.T)\n",
    "    fhiidet=np.flipud(hiidet.T)\n",
    "    fhzzdet=np.flipud(hzzdet.T)\n",
    "    fhyydet=np.flipud(hyydet.T)\n",
    "    img_uud=hv.Image(fhuudet, bounds=(16,16,28,28) ).opts(cmap=\"Blues\",title=\"magU det vs magU true \",xlabel=\"U (mag)\",ylabel=\"U (mag)\").opts(**img_opts)\n",
    "    img_ggd=hv.Image(fhggdet, bounds=(16,16,28,28) ).opts(cmap=\"Greens\",title=\"magG det vs magG true\",xlabel=\"G (mag)\",ylabel=\"G (mag)\").opts(**img_opts)\n",
    "    img_rrd=hv.Image(fhrrdet, bounds=(16,16,28,28) ).opts(cmap=\"Reds\",title=\"magR det vs magR true\",xlabel=\"R (mag)\",ylabel=\"R (mag)\").opts(**img_opts)\n",
    "    img_iid=hv.Image(fhiidet, bounds=(16,16,28,28) ).opts(cmap=\"Oranges\",title=\"magI det vs magI true\",xlabel=\"I (mag)\",ylabel=\"I (mag)\").opts(**img_opts)\n",
    "    img_zzd=hv.Image(fhzzdet, bounds=(16,16,28,28) ).opts(cmap=\"Greys\",title=\"magZ det vs magZ true\",xlabel=\"Z (mag)\",ylabel=\"Z (mag)\").opts(**img_opts)\n",
    "    img_yyd=hv.Image(fhyydet, bounds=(16,16,28,28) ).opts(cmap=\"GnBu\",title=\"magY det vs magY true\",xlabel=\"Y (mag)\",ylabel=\"Y (mag)\").opts(**img_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    layout = img_uud + img_ggd + img_rrd + img_iid + img_zzd + img_yyd\n",
    "    layout.cols(HV_IMAGE_MULTI_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average magnitude vs redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    huz, xhuz, yhuz=np.histogram2d(data.redshift,data.mag_u,bins=(50, 50),range=[[0,3],[16,26]])\n",
    "    hgz, xhgz, yhgz=np.histogram2d(data.redshift,data.mag_g,bins=(50, 50),range=[[0,3],[16,26]])\n",
    "    hrz, xhrz, yhrz=np.histogram2d(data.redshift,data.mag_r,bins=(50, 50),range=[[0,3],[16,26]])\n",
    "    hiz, xhiz, yhiz=np.histogram2d(data.redshift,data.mag_i,bins=(50, 50),range=[[0,3],[16,26]])\n",
    "    hzz, xhzz, yhzz=np.histogram2d(data.redshift,data.mag_z,bins=(50, 50),range=[[0,3],[16,26]])\n",
    "    hyz, xhyz, yhyz=np.histogram2d(data.redshift,data.mag_y,bins=(50, 50),range=[[0,3],[16,26]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_opts = dict(\n",
    "                #height=600, width=700, \n",
    "                xaxis=\"bottom\", \n",
    "                padding = 0.01, fontsize={'title': '8pt'},\n",
    "                #colorbar=True, toolbar='right', show_grid=True,\n",
    "                #aspect='equal',\n",
    "                frame_width= HV_HISTO_MULTI_FRAME_WIDTH,\n",
    "                show_grid=True ,\n",
    "                xlabel=\"redshift\",\n",
    "                #ylabel=\"mag\",\n",
    "                tools=['hover','undo','redo','zoom_in','zoom_out'],\n",
    "                #tools=[myhover,'crosshair'],\n",
    "               )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    fhuz=np.flipud(huz.T)\n",
    "    fhgz=np.flipud(hgz.T)\n",
    "    fhrz=np.flipud(hrz.T)\n",
    "    fhiz=np.flipud(hiz.T)\n",
    "    fhzz=np.flipud(hzz.T)\n",
    "    fhyz=np.flipud(hyz.T)\n",
    "    img_uz=hv.Image(fhuz, bounds=(0,16,3,26) ).opts(cmap=\"Blues\",title=\"magU vs reshift\",ylabel=\"U (mag)\").opts(**img_opts)\n",
    "    img_gz=hv.Image(fhgz, bounds=(0,16,3,26) ).opts(cmap=\"Greens\",title=\"magG vs reshift\",ylabel=\"G (mag)\").opts(**img_opts)\n",
    "    img_rz=hv.Image(fhrz, bounds=(0,16,3,26) ).opts(cmap=\"Reds\",title=\"magR vs reshift\",ylabel=\"R (mag)\").opts(**img_opts)\n",
    "    img_iz=hv.Image(fhiz, bounds=(0,16,3,26) ).opts(cmap=\"Oranges\",title=\"magI vs reshift\",ylabel=\"I (mag)\").opts(**img_opts)\n",
    "    img_zz=hv.Image(fhzz, bounds=(0,16,3,26) ).opts(cmap=\"Greys\",title=\"magZ vs reshift\",ylabel=\"Z (mag)\").opts(**img_opts)\n",
    "    img_yz=hv.Image(fhyz, bounds=(0,16,3,26) ).opts(cmap=\"GnBu\",title=\"magY vs reshift\",ylabel=\"Y (mag)\").opts(**img_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    layout = img_uz + img_gz + img_rz + img_iz + img_zz + img_yz\n",
    "    layout.cols(HV_IMAGE_MULTI_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Color vs redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    humgz, xhumgz, yhumgz=np.histogram2d(data.redshift,data.umg,bins=(50, 50),range=[[0,3],[-0.5,2.]])\n",
    "    hgmrz, xhgmrz, yhgmrz=np.histogram2d(data.redshift,data.gmr,bins=(50, 50),range=[[0,3],[-0.5,2.]])\n",
    "    hrmiz, xhrmiz, yhrmiz=np.histogram2d(data.redshift,data.rmi,bins=(50, 50),range=[[0,3],[-0.5,2.]])\n",
    "    himzz, xhimzz, yhimzz=np.histogram2d(data.redshift,data.imz,bins=(50, 50),range=[[0,3],[-0.5,2.]])\n",
    "    hzmyz, xhzmyz, yhzmyz=np.histogram2d(data.redshift,data.zmy,bins=(50, 50),range=[[0,3],[-0.5,2.]])\n",
    "\n",
    "    fhumgz=np.flipud(humgz.T)\n",
    "    fhgmrz=np.flipud(hgmrz.T)\n",
    "    fhrmiz=np.flipud(hrmiz.T)\n",
    "    fhimzz=np.flipud(himzz.T)\n",
    "    fhzmyz=np.flipud(hzmyz.T)\n",
    "\n",
    "    img_umgz=hv.Image(fhumgz, bounds=(0,-0.5,3,2.) ).opts(cmap=\"Blues\",title=\"U - G vs reshift\",ylabel=\"U-G (mag)\").opts(**img_opts)\n",
    "    img_gmrz=hv.Image(fhgmrz, bounds=(0,-0.5,3,2.) ).opts(cmap=\"Greens\",title=\"G - R vs reshift\",ylabel=\"G-R (mag)\").opts(**img_opts)\n",
    "    img_rmiz=hv.Image(fhrmiz, bounds=(0,-0.5,3,2.) ).opts(cmap=\"Reds\",title=\"R - I vs reshift\",ylabel=\"R-I (mag)\").opts(**img_opts)\n",
    "    img_imzz=hv.Image(fhimzz, bounds=(0,-0.5,3,2.) ).opts(cmap=\"Oranges\",title=\"I - Z vs reshift\",ylabel=\"I-Z (mag)\").opts(**img_opts)\n",
    "    img_zmyz=hv.Image(fhzmyz, bounds=(0,-0.5,3,2.) ).opts(cmap=\"Greys\",title=\"Z - Y vs reshift\",ylabel=\"Z-Y (mag)\").opts(**img_opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    layout = img_umgz + img_gmrz + img_rmiz + img_imzz + img_zmyz \n",
    "    layout.cols(HV_IMAGE_MULTI_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because photo-z's rely on the flux integrated in broad filters, they are more sensitive to broad, dramatic features of the SED. This means that the location of these breaks provides a lot of information for photo-z's. \n",
    "- In particular, the Balmer and 4000 angstrom breaks are in the wavelength range of the LSST filters up to a redshift of ~1.4, \n",
    "- and locating these breaks with the LSST filters provides good leverage for photo-z's (see e.g. Kalmbach et al. 2020 and Malz et al. 2021). \n",
    "- Note that while the Balmer break leaves the LSST filters around z=1.4, \n",
    "- the Lyman break doesn't enter the wavelength range of the LSST filters until about z=2.5 :  This gap in redshift coverage contributes to the degradation of photo-z's at high redshift.\n",
    "\n",
    "- Note that in high-redshift galaxies, photo-z estimators might confuse the Lyman break for the Balmer break. This contributes to the physical degeneracies discussed above in \"What makes photo-z's hard?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    img= Image.open(\"figs/sdss/plot_sdss_filters_2.png\")\n",
    "    img = img.resize((500, 400), Image.ANTIALIAS)\n",
    "    img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Photo-Z estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements from LSST Science book:\n",
    "(https://www.lsst.org/sites/default/files/docs/sciencebook/SB_3.pdf)\n",
    "\n",
    "Photometric redshifts for LSST will be applied and calibrated over the redshift range $0 < z < 4$\n",
    "for galaxies to $r  \\simeq 27.5$. \n",
    "For the majority of science cases, such as weak lensing and BAO, a subset\n",
    "of galaxies with $i < 25.3$ will be used. For this high S/N gold standard subset over the\n",
    "redshift interval, $0 < z < 3$, the photometric redshift requirements are:\n",
    "\n",
    "- The root-mean-square scatter in photometric redshifts, $ \\sigma_z/(1+z)$, must be smaller than 0.05, with a goal of 0.02.\n",
    "- The fraction of $3\\sigma $  outliers at all redshifts must be below 10%.\n",
    "- The bias in $e_z = (z_{photo}−z_{spec})/(1+z_{spec})$ must be below 0.003 (or 0.01 for combined,analyses of weak lensing and baryon acoustic oscillations); \n",
    "- The uncertainty in  $\\sigma_z/(1+z)$ must also be known to similar accuracy.\n",
    "\n",
    "\n",
    "\n",
    "### other definitions\n",
    "\n",
    "- **the photo-z accuracy is the absolute value of the difference between the true and photometric redshifts**.\n",
    "\n",
    "-  **the photo-z uncertainty is the standard deviation of the true redshifts** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "\n",
    "- from DE School IV, University of Oxford, July 18, 2016 :  **Jeff Newman - photometric redshifts for LSST**\n",
    "\n",
    "The tools for PhotoZ evaluation are givenin the notebook and also described in the LSST science book (Performance Chapter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances Evaluation lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function that we will call a lot: makes the zphot/zspec plot and calculates key statistics\n",
    "def plot_lines(zmin=0,zmax=3,zstep=0.05,slope=0.15):\n",
    "    \n",
    "    x = np.arange(zmin,zmax,zstep)\n",
    "    outlier_upper = x + slope*(1+x)\n",
    "    outlier_lower = x - slope*(1+x)\n",
    "\n",
    "    curv_bisect=hv.Curve(zip(x,x)).opts(color=\"red\") \n",
    "    curv_outupper=hv.Curve(zip(x,outlier_upper)).opts(color=\"red\",line_dash='dashed') \n",
    "    curv_outlower=hv.Curve(zip(x,outlier_lower)).opts(color=\"red\",line_dash='dashed') \n",
    "    \n",
    "    layout = curv_bisect * curv_outupper * curv_outlower\n",
    "    return layout\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistic lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(z_spec,z_phot,slope=0.15):\n",
    "    \"\"\"\n",
    "    input : \n",
    "       - z_spec : spectroscopic redshift or true redshift\n",
    "       - z_phot : photo-z reedshift\n",
    "       - slope : slope of line defining the outliers  3 x sigma_z with sigma_z = 5%, so slope = 3 x 0.05 = 0.15 \n",
    "    \"\"\"\n",
    "    \n",
    "    mask = np.abs((z_phot - z_spec)/(1 + z_spec)) > slope\n",
    "    notmask = ~mask \n",
    "    \n",
    "    # Standard Deviation of the predicted redshifts compared to the data:\n",
    "    #-----------------------------------------------------------------\n",
    "    std_result = np.std((z_phot - z_spec)/(1 + z_spec), ddof=1)\n",
    "    print('Standard Deviation: %6.4f' % std_result)\n",
    "    \n",
    "\n",
    "    # Normalized MAD (Median Absolute Deviation):\n",
    "    #------------------------------------------\n",
    "    nmad = 1.48 * np.median(np.abs((z_phot - z_spec)/(1 + z_spec)))\n",
    "    print('Normalized MAD: %6.4f' % nmad)\n",
    "\n",
    "    # Percentage of delta-z > 0.15(1+z) outliers:\n",
    "    #-------------------------------------------\n",
    "    eta = np.sum(np.abs((z_phot - z_spec)/(1 + z_spec)) > 0.15)/len(z_spec)\n",
    "    print('Delta z >0.15(1+z) outliers: %6.3f percent' % (100.*eta))\n",
    "    \n",
    "    # Median offset (normalized by (1+z); i.e., bias:\n",
    "    #-----------------------------------------------\n",
    "    bias = np.median(((z_phot - z_spec)/(1 + z_spec)))\n",
    "    sigbias=std_result/np.sqrt(0.64*len(z_phot))\n",
    "    print('Median offset: %6.3f +/- %6.3f' % (bias,sigbias))\n",
    "    \n",
    "    \n",
    "     # overlay statistics with titles left-aligned and numbers right-aligned\n",
    "    stats_txt = '\\n'.join([\n",
    "        'NMAD  = {:0.2f}'.format(nmad),\n",
    "        'STDEV = {:0.2f}'.format(std_result),\n",
    "        'BIAS  = {:0.2f}'.format(bias),\n",
    "        'ETA   = {:0.2f}'.format(eta)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    return nmad,std_result,bias,eta,stats_txt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The CMNN Photo-z Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CMNNestimator(df,Ncalc = 5000):\n",
    "    \n",
    "\n",
    "\n",
    "    # galaxy true redshifts\n",
    "    data_tz = np.asarray(df['redshift'], dtype='float' )\n",
    "\n",
    "    # galaxy apparent magnitudes\n",
    "    data_m = np.transpose( np.asarray( (df['mag_u'],df['mag_g'],\\\n",
    "                                    df['mag_r'],df['mag_i'],\\\n",
    "                                    df['mag_z'],df['mag_y']),\\\n",
    "                                  dtype='float' ) )\n",
    "\n",
    "    # galaxy apparent magnitude errors\n",
    "    data_me = np.transpose( np.asarray( (df['magerr_u'],df['magerr_g'],\\\n",
    "                                     df['magerr_r'],df['magerr_i'],\\\n",
    "                                     df['magerr_z'],df['magerr_y']),\\\n",
    "                                  dtype='float' ) )\n",
    "    \n",
    "    # galaxy colors and color errors\n",
    "    data_c = np.zeros( (len(data_m),5), dtype='float' )\n",
    "    data_ce = np.zeros( (len(data_m),5), dtype='float' )\n",
    "\n",
    "    data_c[:,0] = data_m[:,0] - data_m[:,1]\n",
    "    data_c[:,1] = data_m[:,1] - data_m[:,2]\n",
    "    data_c[:,2] = data_m[:,2] - data_m[:,3]\n",
    "    data_c[:,3] = data_m[:,3] - data_m[:,4]\n",
    "    data_c[:,4] = data_m[:,4] - data_m[:,5]\n",
    "\n",
    "    data_ce[:,0] = np.sqrt( data_me[:,0]**2 + data_me[:,1]**2 )\n",
    "    data_ce[:,1] = np.sqrt( data_me[:,1]**2 + data_me[:,2]**2 )\n",
    "    data_ce[:,2] = np.sqrt( data_me[:,2]**2 + data_me[:,3]**2 )\n",
    "    data_ce[:,3] = np.sqrt( data_me[:,3]**2 + data_me[:,4]**2 )\n",
    "    data_ce[:,4] = np.sqrt( data_me[:,4]**2 + data_me[:,5]**2 )\n",
    "    \n",
    "    \n",
    "    cmnn_ppf = 0.68 \n",
    "    cmnn_minNclr = 5\n",
    "    \n",
    "    \n",
    "    cmnn_thresh_table = np.zeros( 6, dtype='float' )\n",
    "    for d in range(6):\n",
    "        cmnn_thresh_table[d] = chi2.ppf(cmnn_ppf,d)\n",
    "    cmnn_thresh_table[0] = float(0.0000)\n",
    "\n",
    "    for d in range(6):\n",
    "        print('degrees of freedom, threshold = ',d,cmnn_thresh_table[d])\n",
    "        \n",
    "        \n",
    "    data_pz = np.zeros( len(data_m), dtype='float' ) - 1.0\n",
    "    data_pze = np.zeros( len(data_m), dtype='float' ) - 1.0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    t1 = datetime.datetime.now()\n",
    "\n",
    "    for i in range( Ncalc ):\n",
    "        if (i == 100) | (i == 1000) | (i == Ncalc-1000):\n",
    "            t2 = datetime.datetime.now()\n",
    "            print(i, t2-t1, ((t2-t1)/float(i))*(float(Ncalc)), ' remaining' )\n",
    "        \n",
    "        DM  = np.nansum( ( data_c[i,:] - data_c[:,:] )**2 / data_ce[i,:]**2, axis=1, dtype='float' )\n",
    "        DOF = np.nansum( ( data_c[i,:]**2 + data_c[:,:]**2 + 1.0 ) / ( data_c[i,:]**2 + data_c[:,:]**2 + 1.0 ),axis=1, dtype='int' )\n",
    "    \n",
    "        data_th = np.zeros( len(data_c), dtype='float' )\n",
    "        for d in range(6):\n",
    "            tx = np.where( DOF == d )[0]\n",
    "            data_th[tx] = cmnn_thresh_table[ d ]\n",
    "            del tx\n",
    "    \n",
    "        # reset the Mahalanobis distance for this 'test' galaxy to be very large\n",
    "        # this will \"leave out\" the current 'test' galaxy from the 'training set'\n",
    "        DM[i] = 99.9\n",
    "        \n",
    "        index = np.where( \\\n",
    "        ( DOF >= cmnn_minNclr ) & \\\n",
    "        ( data_th > 0.00010 ) & \\\n",
    "        ( DM > 0.00010 ) & \\\n",
    "        ( DM <= data_th ) )[0]\n",
    "    \n",
    "        if len(index) > 0:\n",
    "            rival = np.random.choice( index, size=1, replace=False )[0]\n",
    "            data_pz[i] = data_tz[rival]\n",
    "            data_pze[i] = np.std( data_tz[index] )\n",
    "            del rival\n",
    "        else:\n",
    "            data_pz[i] = float('nan')\n",
    "            data_pze[i] = float('nan')\n",
    "        \n",
    "        del index, data_th, DOF, DM\n",
    "        \n",
    "        \n",
    "        \n",
    "    tx = np.where( np.isnan(data_pz) )[0]\n",
    "    print( len(tx), ' galaxies did not get a pz estimate' )\n",
    "    del tx\n",
    "\n",
    "    tx = np.where( data_pz > 0.0 )[0]\n",
    "    print( len(tx), ' galaxies did get a pz estimate' )\n",
    "    del tx\n",
    "    \n",
    "    \n",
    "    return data_tz, data_pz,data_pze\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## START ML here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and Feature\n",
    "\n",
    "Because we want to estimate the performance of photoz estimator itself, not the total performance including intrinsic redshift fluctuations. Thus only average magnitudes data will be used : detected magnitude are dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data[\"redshift\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[[\"mag_u\",\"mag_g\",\"mag_r\",\"mag_i\",\"mag_z\",\"mag_y\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total number of samples to split in training, validation and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntot = len(target)\n",
    "Ntot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split in training / test set\n",
    "\n",
    "- speed of the notebook must be tuned with the training sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of samples to be used in training\n",
    "\n",
    "- depending on the required speed of the demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = 10000\n",
    "Ntest = Ntot-Ntrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fraction\n",
    "test_sample_size_fraction=Ntest/Ntot\n",
    "test_sample_size_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt the train dataset size according required running time \n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_sample_size_fraction, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note**\n",
    "- because the model fit (training) may be long, we should limit the training dataset size for this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressors definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear model\n",
    "\n",
    "- Instead of using the LinearRegressor, we start by using the regularized Ridge regressor with the alpha parameter setting the regularization.\n",
    "- Linear model features should be always normalized,\n",
    "- For non linearities, we include the possibility to develop the model as a polynomial of features\n",
    "\n",
    "Scikit-Learn offer to define pipelines of tasks in an easy way:\n",
    "- PolynomialFeatures() task extend features dataset in powers of thos features up to a power degree,\n",
    "- StandardScaler() preprocess the features to normalize them,\n",
    "- Ridge is the regularized version of the LinearRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_regressor = make_pipeline(PolynomialFeatures(degree=4), StandardScaler(),Ridge(alpha=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest regressor\n",
    "\n",
    "- RandomForest regressor is an Enssemble regressor of type Bagging (Bootstrap and Aggregation).  \n",
    "\n",
    "- RandomForest regressor combines the regression of multiple decision tree regressors fitted in parallel \n",
    "on bootstrapped samples from the training sample.\n",
    "\n",
    "- Each individual decision tree is deep, meaning they individually overfit the bootstrapped samples. \n",
    "\n",
    "- The aggregation of the overfitting parallel decision tree model reduce the over-fitting.\n",
    "- For Random Forest, each Decision Tree node feature are drawn randomly. This reduce the error correlation of the various trees.\n",
    "- From this caracteristics, RandomForest is expected to be one of the best non-linear regressor on column-tabulated datasets.\n",
    "\n",
    "\n",
    "- RandomForst includes a number of hyper-parameters.\n",
    "- We use the hyper-parameters chosen byJeff Newmann for the DE-School at Oxford 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "randomforest_regressor = RandomForestRegressor(n_estimators = 50, max_depth = 30, max_features = 'auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor\n",
    "\n",
    "The INRIA MOOC (2022) on scikit-Learn ( Machine learning in Python with scikit-learn: https://lms.fun-mooc.fr/courses/course-v1:inria+41026+session02/info)\n",
    "recommend the histogram-binned version of GradientBoostingRegressor, expected to have a good balance between underfitting and overfitting.\n",
    "\n",
    "- Boosting Regressor perform shallow Decision trees (underfitting) fit sequencially. The subsequent Decision Tree fitting improve the fit quality.\n",
    "Among the set of bossting regressors, Gradient Boosting Regressor is expected to decrease the bias but avoid overfitting. Among them the Histogram Gradient Boosting regressor is expected to run faster \n",
    "\n",
    "- To increase the fitting time, we allow here early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "#from sklearn.preprocessing import KBinsDiscretizer\n",
    "#discretizer = KBinsDiscretizer(n_bins=64, encode=\"ordinal\", strategy=\"quantile\")\n",
    "#histogram_gradient_boosting_regressor = make_pipeline(discretizer, HistGradientBoostingRegressor(max_iter=30))\n",
    "histogram_gradient_boosting_regressor =  HistGradientBoostingRegressor(max_iter=20,early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "scoring = {'r2': make_scorer(r2_score),'mae': make_scorer(mean_absolute_error),'mse': make_scorer(mean_squared_error)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of cross-validation\n",
    "\n",
    "- We use cross-validation to select a sub-sample of galaxies from the complete training sample and train the model with this subset.\n",
    "- This sub-sampling is repeated several times (n_split=5).\n",
    "\n",
    "The interest of this multi-subsampling is to have a set of almost similar but slightly different fitted models from which we can derive several predictions for a test sample, thus an average predicted value and its variation (or a PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't know of the galaxies are ordered by redshift or come randomly. Thus we activate a pre-random-shuffling in the training dataset. \n",
    "cv = ShuffleSplit(n_splits=5, test_size=.80, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge model\n",
    "\n",
    "- The cross_validate function performs fit on n_splits models from n_splits random subsamples\n",
    "- The smaple is previously randomized\n",
    "- The evaluation metric is given by the scoring (The INRIA MOOC use this coring=\"neg_mean_absolute_error\"),\n",
    "- The n_splits fitted models are retuned (to be able to make n_split prediction for a single test sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t1 = datetime.datetime.now()\n",
    "cv_results = cross_validate(ridge_regressor,X_train,y_train,cv=cv,scoring=scoring,return_estimator=True)\n",
    "t2 = datetime.datetime.now()\n",
    "deltat = (t2-t1).total_seconds() \n",
    "print(f\"Ridge CV : elapsed time {deltat:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results = pd.DataFrame(cv_results)\n",
    "df_cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one of the n_splits model, but all predictions for all estimators could be calculated (average and rms)\n",
    "y_pred = cv_results[\"estimator\"][0].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = zip(y_test,y_pred)\n",
    "points = hv.Points(coords).opts(tools=['box_select', 'lasso_select'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmad,std_result,bias,eta,stats_txt1 = get_stats(y_test.values,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a holoviews object to hold and plot data\n",
    "# Create the linked streams instance\n",
    "boundsxy = (0, 0, 0, 0)\n",
    "box = streams.BoundsXY(source=points, bounds=boundsxy)\n",
    "bounds = hv.DynamicMap(lambda bounds: hv.Bounds(bounds), streams=[box])\n",
    "\n",
    "# Apply the datashader\n",
    "p1 = dynspread(datashade(points, cmap=\"Viridis\"))\n",
    "p1 = p1.opts(width=HV_HISTO_SINGLE_WIDTH, height=HV_HISTO_SINGLE_HEIGHT,\n",
    "    padding=0.05, show_grid=True,\n",
    "    xlim=(0, 3), ylim=(0, 3.0),\n",
    "    xlabel=\"z-spec\", ylabel=\"z-phot\",title=\"Ridge Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 * plot_lines() * hv.Text(0.5, 2.5, stats_txt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_r2   = f\"R2 score : \\t\\t {df_cv_results['test_r2'].mean():.3f} +/-  {df_cv_results['test_r2'].std():.3f}\"\n",
    "msg_mae  = f\"MAE mean absolute error : \\t {df_cv_results['test_mae'].mean():.3f} +/-  {df_cv_results['test_mae'].std():.3f}\"\n",
    "msg_rmsq = f\"Root MSE error : \\t\\t {np.sqrt(df_cv_results['test_mse'].mean()):.3f} +/-  {np.sqrt(df_cv_results['test_mse'].std()):.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(msg_r2)\n",
    "print(msg_mae)\n",
    "print(msg_rmsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "- take the hyper-parameter of the DE school"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select smaller training sample\n",
    "\n",
    "- faster model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = 8000\n",
    "Ntest = Ntot-Ntrain\n",
    "test_sample_size_fraction=Ntest/Ntot\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_sample_size_fraction, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_results = cross_validate(randomforest_regressor ,X_train,y_train,cv=cv,scoring=scoring,return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# We simply use the fit method, not the cross_validate to accelerate the demo \n",
    "t1 = datetime.datetime.now()\n",
    "randomforest_regressor.fit(X_train,y_train)\n",
    "t2 = datetime.datetime.now()\n",
    "deltat = (t2-t1).total_seconds() \n",
    "print(f\"RandomForest : elapsed time {deltat:.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =  randomforest_regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmad,std_result,bias,eta,stats_txt2= get_stats(y_test.values,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = zip(y_test,y_pred)\n",
    "points = hv.Points(coords).opts(tools=['box_select', 'lasso_select'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundsxy = (0, 0, 0, 0)\n",
    "box = streams.BoundsXY(source=points, bounds=boundsxy)\n",
    "bounds = hv.DynamicMap(lambda bounds: hv.Bounds(bounds), streams=[box])\n",
    "\n",
    "# Apply the datashader\n",
    "p2 = dynspread(datashade(points, cmap=\"Viridis\"))\n",
    "p2 = p2.opts(width=HV_HISTO_SINGLE_WIDTH, height=HV_HISTO_SINGLE_HEIGHT,\n",
    "    padding=0.05, show_grid=True,\n",
    "    xlim=(0, 3), ylim=(0, 3.0),\n",
    "    xlabel=\"z-spec\", ylabel=\"z-phot\",title=\"Random Forest Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 * plot_lines() *  hv.Text(0.5, 2.5, stats_txt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics in scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2  = r2_score(y_pred,y_test)\n",
    "mae = mean_absolute_error(y_pred,y_test)\n",
    "mse = mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_r2   = f\"R2 score : \\t\\t {r2:.3f}\"\n",
    "msg_mae  = f\"MAE mean absolute error : \\t {mae:.3f}\"\n",
    "msg_rmsq = f\"Root MSE error : \\t\\t {np.sqrt(mse):.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(msg_r2)\n",
    "print(msg_mae)\n",
    "print(msg_rmsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Gradient Boosting regressor\n",
    "\n",
    "- No particular optimisation of hyper parameters performed here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select smaller training sample\n",
    "\n",
    "- faster model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = 8000\n",
    "Ntest = Ntot-Ntrain\n",
    "test_sample_size_fraction=Ntest/Ntot\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_sample_size_fraction, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training (model fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# We simply use the fit method, not the cross_validate to accelerate the demo \n",
    "t1 = datetime.datetime.now()\n",
    "histogram_gradient_boosting_regressor.fit(X_train,y_train) \n",
    "t2 = datetime.datetime.now()\n",
    "deltat = (t2-t1).total_seconds() \n",
    "print(f\"Histogram Gradient Boosting : elapsed time {deltat:.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =  histogram_gradient_boosting_regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = zip(y_test,y_pred)\n",
    "points = hv.Points(coords).opts(tools=['box_select', 'lasso_select'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmad,std_result,bias,eta,stats_txt3 = get_stats(y_test.values,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundsxy = (0, 0, 0, 0)\n",
    "box = streams.BoundsXY(source=points, bounds=boundsxy)\n",
    "bounds = hv.DynamicMap(lambda bounds: hv.Bounds(bounds), streams=[box])\n",
    "\n",
    "# Apply the datashader\n",
    "p3 = dynspread(datashade(points, cmap=\"Viridis\"))\n",
    "p3 = p3.opts(width=HV_HISTO_SINGLE_WIDTH, height=HV_HISTO_SINGLE_HEIGHT,\n",
    "    padding=0.05, show_grid=True,\n",
    "    xlim=(0, 3), ylim=(0, 3.0),\n",
    "    xlabel=\"z-spec\", ylabel=\"z-phot\",title=\"Histogram Gradient Boosting Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 * plot_lines() *  hv.Text(0.5, 2.5, stats_txt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics in scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2  = r2_score(y_pred,y_test)\n",
    "mae = mean_absolute_error(y_pred,y_test)\n",
    "mse = mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_r2   = f\"R2 score : \\t\\t {r2:.3f}\"\n",
    "msg_mae  = f\"MAE mean absolute error : \\t {mae:.3f}\"\n",
    "msg_rmsq = f\"Root MSE error : \\t\\t {np.sqrt(mse):.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(msg_r2)\n",
    "print(msg_mae)\n",
    "print(msg_rmsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CMNN Photo-z Estimator\n",
    "\n",
    "\n",
    "We want to compare the above ML algorithm with the CMNN leave-one-out estimator propose by Melissa Graham in another DP0 contributed notebook.\n",
    "\n",
    "A full description of the Color-Matched Nearest-Neighbors (CMNN) Photometric Redshift Estimator can be found in the following journal articles:\n",
    " * <a href=\"https://ui.adsabs.harvard.edu/abs/2018AJ....155....1G/abstract\">Photometric Redshifts with the LSST: Evaluating Survey Observing Strategies</a> (Graham et al. 2018) \n",
    " * <a href=\"https://ui.adsabs.harvard.edu/abs/2020AJ....159..258G/abstract\">Photometric Redshifts with the LSST. II. The Impact of Near-infrared and Near-ultraviolet Photometry</a> (Graham et al. 2020)\n",
    "\n",
    "The CMNN PZ Estimator can also be found on GitHub: https://github.com/dirac-institute/CMNN_Photoz_Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmnn_tz, cmnn_pz, cmnn_pze = CMNNestimator(data,Ncalc = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = np.where(cmnn_pz > 0.0 )[0]\n",
    "\n",
    "cmnn_tz_sel=cmnn_tz[tx]\n",
    "cmnn_pz_sel=cmnn_pz[tx] \n",
    "cmnn_pze_sel =  cmnn_pze[tx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = zip(cmnn_tz_sel,cmnn_pz_sel)\n",
    "points = hv.Points(coords).opts(tools=['box_select', 'lasso_select'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmad,std_result,bias,eta,stats_txt4 = get_stats(cmnn_tz_sel,cmnn_pz_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundsxy = (0, 0, 0, 0)\n",
    "box = streams.BoundsXY(source=points, bounds=boundsxy)\n",
    "bounds = hv.DynamicMap(lambda bounds: hv.Bounds(bounds), streams=[box])\n",
    "\n",
    "# Apply the datashader\n",
    "p4 = dynspread(datashade(points, cmap=\"Viridis\"))\n",
    "p4 = p4.opts(width=HV_HISTO_SINGLE_WIDTH, height=HV_HISTO_SINGLE_HEIGHT,\n",
    "    padding=0.05, show_grid=True,\n",
    "    xlim=(0, 3), ylim=(0, 3.0),\n",
    "    xlabel=\"z-spec\", ylabel=\"z-phot\",title=\"CMNN estimator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 * plot_lines() *  hv.Text(0.5, 2.5, stats_txt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The photo-z accuracy is the absolute value of the difference between the true and photometric redshifts**.\n",
    "\n",
    "Recall that **the photo-z uncertainty is the standard deviation of the true redshifts**  of the training-set galaxies in the CMNN subset. The fact that a bunch of galaxies have an uncertainty of zero means there are galaxies with only 1 training-set galaxy in their CMNN subset. The full CMNN PZ Estimator treats such galaxies better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should we use uncertainty-accuracy normalized by (1+z) or not ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias=np.abs(cmnn_tz_sel-cmnn_pz_sel)\n",
    "err= cmnn_pze_sel\n",
    "coords1 = zip(bias,err)\n",
    "points1 = hv.Points(coords1).opts(tools=['box_select', 'lasso_select'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_norm = np.abs(cmnn_tz_sel-cmnn_pz_sel)/(1+cmnn_tz_sel)\n",
    "err_norm= cmnn_pze_sel/(1+cmnn_tz_sel)\n",
    "coords2 = zip(bias_norm,err_norm)\n",
    "points2 = hv.Points(coords2).opts(tools=['box_select', 'lasso_select'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the datashader\n",
    "p5 = dynspread(datashade(points1, cmap=\"Viridis\"))\n",
    "p5 = p5.opts(width=HV_HISTO_SINGLE_WIDTH//2, height=HV_HISTO_SINGLE_HEIGHT//2,\n",
    "    padding=0.05, show_grid=True,xlim=(0,1),ylim=(0,1),\n",
    "    xlabel=\"accuracy\", ylabel=\"uncertainty\",title=\"CMNN estimator : uncertainty vs accuracy \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the datashader\n",
    "p6 = dynspread(datashade(points2, cmap=\"Viridis\"))\n",
    "p6 = p6.opts(width=HV_HISTO_SINGLE_WIDTH//2, height=HV_HISTO_SINGLE_HEIGHT//2,\n",
    "    padding=0.05, show_grid=True,xlim=(0,1),ylim=(0,1),\n",
    "    xlabel=\"accuracy\", ylabel=\"uncertainty\",title=\"CMNN estimator : norm-uncertainty vs norm-accuracy \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p5 + p6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work : Optimisation of hyper parameters\n",
    "\n",
    "- TBD later in another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Clean the output directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_CLEAN_DATAONDISK:\n",
    "    if os.path.isdir(TMPNBDIR):\n",
    "        try:\n",
    "            shutil.rmtree(TMPNBDIR)\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s : %s\" % (TMPNBDIR, e.strerror)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
